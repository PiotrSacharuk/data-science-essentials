{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3968636",
   "metadata": {},
   "source": [
    "# URL Data Loading Test\n",
    "\n",
    "**Author:** Data Science Essentials Project  \n",
    "**Date:** September 23, 2025  \n",
    "**Purpose:** Testing URL data loading functionality with automatic caching\n",
    "\n",
    "This notebook demonstrates and tests the new URL data loading functionality added to the PandasSource class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed8bfe",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Before running this notebook**, make sure you have:\n",
    "\n",
    "- Set up the project environment with required dependencies\n",
    "- Network access for downloading test datasets from web URLs\n",
    "- The updated `PandasSource` class with URL support\n",
    "\n",
    "**Features Tested:**\n",
    "- Direct CSV loading from web URLs\n",
    "- Automatic file caching with concurrent access safety\n",
    "- Performance improvements from caching\n",
    "- Error handling for invalid URLs\n",
    "- Support for datasets with and without headers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to Python path\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Find the project root - handle both local and CI environments\n",
    "notebook_dir = Path(os.getcwd())\n",
    "if notebook_dir.name == 'exploratory' and notebook_dir.parent.name == 'notebooks':\n",
    "    # Running notebook directly in its folder\n",
    "    project_root = notebook_dir.parent.parent\n",
    "else:\n",
    "    # CI environment or other directory\n",
    "    for possible_root in [Path(os.getcwd()), Path(os.getcwd()).parent]:\n",
    "        if (possible_root / 'notebooks' / 'exploratory').exists():\n",
    "            project_root = possible_root\n",
    "            break\n",
    "    else:\n",
    "        # Fallback to relative path from notebook\n",
    "        project_root = Path('.').absolute().parent.parent\n",
    "\n",
    "# Add to Python path if not already there\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Import PandasSource from project\n",
    "from src.data.sources.pandas_source import PandasSource\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf50403",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a06d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test URL - Iris dataset from UCI repository\n",
    "iris_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "\n",
    "# Define column names for Iris dataset (it has no header)\n",
    "iris_column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "\n",
    "print(f\"Loading data from: {iris_url}\")\n",
    "\n",
    "# Measure download time\n",
    "start_time = time.time()\n",
    "data_source = PandasSource(\n",
    "    file_path=iris_url,\n",
    "    header=False,  # No header in the file\n",
    "    names=iris_column_names\n",
    ")\n",
    "download_time = time.time() - start_time\n",
    "\n",
    "print(f\"Data loaded successfully in {download_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic dataset information\n",
    "print(f\"Dataset shape: {data_source.df.shape}\")\n",
    "print(f\"Dataset columns: {list(data_source.df.columns)}\")\n",
    "data_source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34734272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics\n",
    "data_source.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134e21c",
   "metadata": {},
   "source": [
    "## 3. Caching Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93049340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the same URL again - should use cached version\n",
    "start_time = time.time()\n",
    "data_source_cached = PandasSource(\n",
    "    file_path=iris_url,\n",
    "    header=False,\n",
    "    names=iris_column_names\n",
    ")\n",
    "cached_time = time.time() - start_time\n",
    "\n",
    "print(f\"Cached load time: {cached_time:.4f} seconds\")\n",
    "print(f\"Speed improvement: {download_time/cached_time:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cached data integrity\n",
    "data_integrity_check = data_source.df.equals(data_source_cached.df)\n",
    "print(f\"Data integrity verified: {data_integrity_check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c022d860",
   "metadata": {},
   "source": [
    "## 4. Cache Management and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display source metadata\n",
    "data_source.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c882710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine cache directory\n",
    "cache_dir = Path(data_source.cache_dir)\n",
    "if cache_dir.exists():\n",
    "    csv_files = list(cache_dir.glob(\"*.csv\"))\n",
    "    print(f\"Cache directory: {cache_dir}\")\n",
    "    print(f\"Cached files: {len(csv_files)}\")\n",
    "    for file in csv_files:\n",
    "        print(f\"  - {file.name} ({file.stat().st_size} bytes)\")\n",
    "else:\n",
    "    print(f\"Cache directory not found: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80bd64",
   "metadata": {},
   "source": [
    "## 5. Error Handling and Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13bebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error handling for invalid URLs\n",
    "error_test_cases = [\n",
    "    (\"https://nonexistent-domain-12345.com/data.csv\", \"Non-existent domain\"),\n",
    "    (\"not-a-url\", \"Invalid URL format\"),\n",
    "    (\"\", \"Empty string\"),\n",
    "]\n",
    "\n",
    "for test_url, description in error_test_cases:\n",
    "    try:\n",
    "        test_source = PandasSource(file_path=test_url)\n",
    "        print(f\"{description}: Unexpected success\")\n",
    "    except Exception as e:\n",
    "        print(f\"{description}: {type(e).__name__} (Expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f83e32",
   "metadata": {},
   "source": [
    "## 6. Multiple Data Sources Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple data sources simultaneously\n",
    "test_urls = [\n",
    "    {\n",
    "        \"name\": \"Iris Dataset\", \n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n",
    "        \"columns\": ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'],\n",
    "        \"header\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Tips Dataset\",\n",
    "        \"url\": \"https://raw.githubusercontent.com/plotly/datasets/master/tips.csv\", \n",
    "        \"columns\": None,\n",
    "        \"header\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "successful_loads = 0\n",
    "total_tests = len(test_urls)\n",
    "\n",
    "for i, test_case in enumerate(test_urls, 1):\n",
    "    try:\n",
    "        print(f\"Loading {test_case['name']}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        source = PandasSource(\n",
    "            file_path=test_case['url'],\n",
    "            header=test_case['header'],\n",
    "            names=test_case['columns'] if not test_case['header'] else None\n",
    "        )\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"  ✓ Shape: {source.df.shape}\")\n",
    "        print(f\"  ✓ Columns: {list(source.df.columns)[:3]}{'...' if len(source.df.columns) > 3 else ''}\")\n",
    "        print(f\"  ✓ Load time: {load_time:.3f}s\")\n",
    "        print()\n",
    "        \n",
    "        successful_loads += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed: {type(e).__name__}: {e}\")\n",
    "        print()\n",
    "\n",
    "print(f\"Summary: {successful_loads}/{total_tests} datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test concurrent access to cache (simulate multiple processes)\n",
    "import threading\n",
    "\n",
    "def load_iris_data(thread_id):\n",
    "    \"\"\"Load iris data in a separate thread to test concurrent cache access\"\"\"\n",
    "    try:\n",
    "        iris_data = PandasSource(\n",
    "            file_path=iris_url,\n",
    "            header=False,\n",
    "            names=iris_column_names\n",
    "        )\n",
    "        return f\"Thread {thread_id}: Success - Shape {iris_data.df.shape}\"\n",
    "    except Exception as e:\n",
    "        return f\"Thread {thread_id}: Error - {e}\"\n",
    "\n",
    "# Launch multiple threads to test concurrent cache access\n",
    "threads = []\n",
    "results = []\n",
    "\n",
    "print(\"Testing concurrent cache access...\")\n",
    "for i in range(3):\n",
    "    thread = threading.Thread(target=lambda i=i: results.append(load_iris_data(i+1)))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print(\"Concurrent access results:\")\n",
    "for result in results:\n",
    "    print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a04b5",
   "metadata": {},
   "source": [
    "## 7. Cache Refresh Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e829423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cache refresh functionality\n",
    "print(f\"Before refresh - cached file exists: {data_source.file_path.exists()}\")\n",
    "\n",
    "data_source.refresh_cache()\n",
    "\n",
    "print(f\"After refresh - cached file exists: {data_source.file_path.exists()}\")\n",
    "print(f\"Data shape after refresh: {data_source.df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748832b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading from GitHub datasets (with headers)\n",
    "tips_url = \"https://raw.githubusercontent.com/plotly/datasets/master/tips.csv\"\n",
    "\n",
    "tips_source = PandasSource(file_path=tips_url, header=True)\n",
    "print(f\"Tips dataset shape: {tips_source.df.shape}\")\n",
    "print(f\"Tips dataset columns: {list(tips_source.df.columns)}\")\n",
    "tips_source.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62f230",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "This notebook successfully demonstrated the URL data loading functionality in PandasSource:\n",
    "\n",
    "- **Basic URL Loading**: Direct CSV loading from web URLs with proper column handling\n",
    "- **Caching Performance**: Significant speed improvements for repeated access\n",
    "- **Data Integrity**: Cached data maintains perfect consistency with original\n",
    "- **Error Handling**: Robust handling of invalid URLs and network errors\n",
    "- **Multiple Sources**: Support for different CSV formats (with/without headers)\n",
    "- **Cache Management**: Automatic file caching with refresh capabilities\n",
    "\n",
    "The implementation provides a seamless API for both local files and remote URLs, making it suitable for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test refresh_cache error handling for local files\n",
    "try:\n",
    "    local_file_path = project_root / \"data\" / \"raw\" / \"iris.csv\"\n",
    "    local_source = PandasSource(file_path=str(local_file_path))\n",
    "    local_source.refresh_cache()\n",
    "    print(\"Unexpected success with local file\")\n",
    "except ValueError as e:\n",
    "    print(f\"Expected error for local file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
